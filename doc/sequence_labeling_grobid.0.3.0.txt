citation model
==============

> python3 delft/applications/grobidTagger.py citation train_eval --architecture BidLSTM_CRF --embedding glove-840B --fold-count 10 --input data/sequenceLabelling/grobid/citation/citation-060518.train

training runtime: 40340.819 seconds 

Evaluation:

------------------------ fold 0 --------------------------------------
    f1 (micro): 94.82
                  precision    recall  f1-score   support

        <author>     0.9406    0.9421    0.9414       639
     <booktitle>     0.7500    0.7373    0.7436       118
 <collaboration>     1.0000    0.7500    0.8571        12
          <date>     0.9843    0.9843    0.9843       699
        <editor>     0.6429    0.6429    0.6429        14
   <institution>     0.7143    0.7895    0.7500        19
         <issue>     0.9275    0.9143    0.9209        70
       <journal>     0.9376    0.9516    0.9445       537
      <location>     0.8778    0.8404    0.8587        94
          <note>     0.8485    0.7179    0.7778        39
         <pages>     0.9793    0.9844    0.9818       576
     <publisher>     0.9216    0.9400    0.9307        50
        <pubnum>     0.9588    0.9118    0.9347       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7222    0.7647    0.7429        17
         <title>     0.9606    0.9606    0.9606       457
        <volume>     0.9632    0.9850    0.9740       532
           <web>     0.9167    0.9167    0.9167        12

all (micro avg.)     0.9472    0.9491    0.9482      3989


------------------------ fold 1 --------------------------------------
    f1 (micro): 95.05
                  precision    recall  f1-score   support

        <author>     0.9498    0.9484    0.9491       639
     <booktitle>     0.7895    0.7627    0.7759       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9900    0.9886    0.9893       699
        <editor>     0.7692    0.7143    0.7407        14
   <institution>     0.7619    0.8421    0.8000        19
         <issue>     0.9155    0.9286    0.9220        70
       <journal>     0.9353    0.9423    0.9388       537
      <location>     0.8791    0.8511    0.8649        94
          <note>     0.8182    0.6923    0.7500        39
         <pages>     0.9811    0.9896    0.9853       576
     <publisher>     0.9388    0.9200    0.9293        50
        <pubnum>     0.9485    0.9020    0.9246       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7000    0.8235    0.7568        17
         <title>     0.9542    0.9584    0.9563       457
        <volume>     0.9631    0.9812    0.9721       532
           <web>     0.8462    0.9167    0.8800        12

all (micro avg.)     0.9499    0.9511    0.9505      3989


------------------------ fold 2 --------------------------------------
    f1 (micro): 95.42
                  precision    recall  f1-score   support

        <author>     0.9561    0.9546    0.9554       639
     <booktitle>     0.7642    0.7966    0.7801       118
 <collaboration>     0.9091    0.8333    0.8696        12
          <date>     0.9843    0.9857    0.9850       699
        <editor>     0.6923    0.6429    0.6667        14
   <institution>     0.7727    0.8947    0.8293        19
         <issue>     0.9429    0.9429    0.9429        70
       <journal>     0.9461    0.9479    0.9470       537
      <location>     0.9043    0.9043    0.9043        94
          <note>     0.7576    0.6410    0.6944        39
         <pages>     0.9793    0.9878    0.9836       576
     <publisher>     0.9592    0.9400    0.9495        50
        <pubnum>     0.9697    0.9412    0.9552       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.6667    0.7059    0.6857        17
         <title>     0.9604    0.9562    0.9583       457
        <volume>     0.9686    0.9850    0.9767       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9530    0.9554    0.9542      3989


------------------------ fold 3 --------------------------------------
    f1 (micro): 95.11
                  precision    recall  f1-score   support

        <author>     0.9545    0.9531    0.9538       639
     <booktitle>     0.7563    0.7627    0.7595       118
 <collaboration>     0.9091    0.8333    0.8696        12
          <date>     0.9871    0.9857    0.9864       699
        <editor>     0.6429    0.6429    0.6429        14
   <institution>     0.8000    0.8421    0.8205        19
         <issue>     0.9559    0.9286    0.9420        70
       <journal>     0.9357    0.9479    0.9417       537
      <location>     0.8804    0.8617    0.8710        94
          <note>     0.8333    0.6410    0.7246        39
         <pages>     0.9827    0.9861    0.9844       576
     <publisher>     0.9592    0.9400    0.9495        50
        <pubnum>     0.9300    0.9118    0.9208       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7647    0.7647    0.7647        17
         <title>     0.9582    0.9540    0.9561       457
        <volume>     0.9650    0.9850    0.9749       532
           <web>     0.8333    0.8333    0.8333        12

all (micro avg.)     0.9511    0.9511    0.9511      3989


------------------------ fold 4 --------------------------------------
    f1 (micro): 95.33
                  precision    recall  f1-score   support

        <author>     0.9422    0.9437    0.9429       639
     <booktitle>     0.7750    0.7881    0.7815       118
 <collaboration>     0.9091    0.8333    0.8696        12
          <date>     0.9871    0.9871    0.9871       699
        <editor>     0.7692    0.7143    0.7407        14
   <institution>     0.7619    0.8421    0.8000        19
         <issue>     0.9412    0.9143    0.9275        70
       <journal>     0.9414    0.9572    0.9492       537
      <location>     0.9121    0.8830    0.8973        94
          <note>     0.8529    0.7436    0.7945        39
         <pages>     0.9793    0.9878    0.9836       576
     <publisher>     0.9388    0.9200    0.9293        50
        <pubnum>     0.9485    0.9020    0.9246       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.6500    0.7647    0.7027        17
         <title>     0.9647    0.9562    0.9604       457
        <volume>     0.9705    0.9887    0.9795       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9522    0.9544    0.9533      3989


------------------------ fold 5 --------------------------------------
    f1 (micro): 95.00
                  precision    recall  f1-score   support

        <author>     0.9437    0.9452    0.9445       639
     <booktitle>     0.7623    0.7881    0.7750       118
 <collaboration>     0.9091    0.8333    0.8696        12
          <date>     0.9843    0.9857    0.9850       699
        <editor>     0.6364    0.5000    0.5600        14
   <institution>     0.7143    0.7895    0.7500        19
         <issue>     0.9420    0.9286    0.9353        70
       <journal>     0.9458    0.9423    0.9440       537
      <location>     0.9111    0.8723    0.8913        94
          <note>     0.8065    0.6410    0.7143        39
         <pages>     0.9793    0.9844    0.9818       576
     <publisher>     0.9057    0.9600    0.9320        50
        <pubnum>     0.9691    0.9216    0.9447       102
        <series>     0.5000    0.5000    0.5000         2
          <tech>     0.6190    0.7647    0.6842        17
         <title>     0.9626    0.9562    0.9594       457
        <volume>     0.9651    0.9868    0.9758       532
           <web>     0.8333    0.8333    0.8333        12

all (micro avg.)     0.9496    0.9504    0.9500      3989


------------------------ fold 6 --------------------------------------
    f1 (micro): 95.31
                  precision    recall  f1-score   support

        <author>     0.9577    0.9577    0.9577       639
     <booktitle>     0.7731    0.7797    0.7764       118
 <collaboration>     1.0000    0.7500    0.8571        12
          <date>     0.9871    0.9814    0.9842       699
        <editor>     0.6429    0.6429    0.6429        14
   <institution>     0.8500    0.8947    0.8718        19
         <issue>     0.9412    0.9143    0.9275        70
       <journal>     0.9426    0.9479    0.9452       537
      <location>     0.8696    0.8511    0.8602        94
          <note>     0.7576    0.6410    0.6944        39
         <pages>     0.9793    0.9861    0.9827       576
     <publisher>     0.9592    0.9400    0.9495        50
        <pubnum>     0.9400    0.9216    0.9307       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7222    0.7647    0.7429        17
         <title>     0.9669    0.9584    0.9626       457
        <volume>     0.9703    0.9831    0.9767       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9540    0.9521    0.9531      3989


------------------------ fold 7 --------------------------------------
    f1 (micro): 94.70
                  precision    recall  f1-score   support

        <author>     0.9467    0.9452    0.9460       639
     <booktitle>     0.7063    0.7542    0.7295       118
 <collaboration>     1.0000    0.7500    0.8571        12
          <date>     0.9843    0.9886    0.9864       699
        <editor>     0.5625    0.6429    0.6000        14
   <institution>     0.6500    0.6842    0.6667        19
         <issue>     0.9692    0.9000    0.9333        70
       <journal>     0.9442    0.9460    0.9451       537
      <location>     0.8681    0.8404    0.8541        94
          <note>     0.8387    0.6667    0.7429        39
         <pages>     0.9777    0.9878    0.9827       576
     <publisher>     0.8824    0.9000    0.8911        50
        <pubnum>     0.9495    0.9216    0.9353       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7647    0.7647    0.7647        17
         <title>     0.9559    0.9497    0.9528       457
        <volume>     0.9668    0.9850    0.9758       532
           <web>     0.8462    0.9167    0.8800        12

all (micro avg.)     0.9462    0.9479    0.9470      3989


------------------------ fold 8 --------------------------------------
    f1 (micro): 95.12
                  precision    recall  f1-score   support

        <author>     0.9577    0.9577    0.9577       639
     <booktitle>     0.7266    0.7881    0.7561       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9857    0.9871    0.9864       699
        <editor>     0.6923    0.6429    0.6667        14
   <institution>     0.7500    0.7895    0.7692        19
         <issue>     0.9275    0.9143    0.9209        70
       <journal>     0.9390    0.9460    0.9425       537
      <location>     0.8667    0.8298    0.8478        94
          <note>     0.7941    0.6923    0.7397        39
         <pages>     0.9776    0.9861    0.9818       576
     <publisher>     0.9423    0.9800    0.9608        50
        <pubnum>     0.9588    0.9118    0.9347       102
        <series>     1.0000    0.5000    0.6667         2
          <tech>     0.6190    0.7647    0.6842        17
         <title>     0.9621    0.9431    0.9525       457
        <volume>     0.9723    0.9887    0.9804       532
           <web>     0.9167    0.9167    0.9167        12

all (micro avg.)     0.9502    0.9521    0.9512      3989


------------------------ fold 9 --------------------------------------
    f1 (micro): 94.66
                  precision    recall  f1-score   support

        <author>     0.9544    0.9499    0.9522       639
     <booktitle>     0.7653    0.6356    0.6944       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9801    0.9843    0.9822       699
        <editor>     0.6154    0.5714    0.5926        14
   <institution>     0.7619    0.8421    0.8000        19
         <issue>     0.9412    0.9143    0.9275        70
       <journal>     0.9375    0.9497    0.9436       537
      <location>     0.8723    0.8723    0.8723        94
          <note>     0.8710    0.6923    0.7714        39
         <pages>     0.9760    0.9896    0.9828       576
     <publisher>     0.9216    0.9400    0.9307        50
        <pubnum>     0.9583    0.9020    0.9293       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.6842    0.7647    0.7222        17
         <title>     0.9258    0.9562    0.9408       457
        <volume>     0.9668    0.9850    0.9758       532
           <web>     0.8462    0.9167    0.8800        12

all (micro avg.)     0.9453    0.9479    0.9466      3989

----------------------------------------------------------------------

** Worst ** model scores - run 9
                  precision    recall  f1-score   support

        <author>     0.9544    0.9499    0.9522       639
     <booktitle>     0.7653    0.6356    0.6944       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9801    0.9843    0.9822       699
        <editor>     0.6154    0.5714    0.5926        14
   <institution>     0.7619    0.8421    0.8000        19
         <issue>     0.9412    0.9143    0.9275        70
       <journal>     0.9375    0.9497    0.9436       537
      <location>     0.8723    0.8723    0.8723        94
          <note>     0.8710    0.6923    0.7714        39
         <pages>     0.9760    0.9896    0.9828       576
     <publisher>     0.9216    0.9400    0.9307        50
        <pubnum>     0.9583    0.9020    0.9293       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.6842    0.7647    0.7222        17
         <title>     0.9258    0.9562    0.9408       457
        <volume>     0.9668    0.9850    0.9758       532
           <web>     0.8462    0.9167    0.8800        12

all (micro avg.)     0.9453    0.9479    0.9466      3989


** Best ** model scores - run 2
                  precision    recall  f1-score   support

        <author>     0.9561    0.9546    0.9554       639
     <booktitle>     0.7642    0.7966    0.7801       118
 <collaboration>     0.9091    0.8333    0.8696        12
          <date>     0.9843    0.9857    0.9850       699
        <editor>     0.6923    0.6429    0.6667        14
   <institution>     0.7727    0.8947    0.8293        19
         <issue>     0.9429    0.9429    0.9429        70
       <journal>     0.9461    0.9479    0.9470       537
      <location>     0.9043    0.9043    0.9043        94
          <note>     0.7576    0.6410    0.6944        39
         <pages>     0.9793    0.9878    0.9836       576
     <publisher>     0.9592    0.9400    0.9495        50
        <pubnum>     0.9697    0.9412    0.9552       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.6667    0.7059    0.6857        17
         <title>     0.9604    0.9562    0.9583       457
        <volume>     0.9686    0.9850    0.9767       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9530    0.9554    0.9542      3989

----------------------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

        <author>     0.9504    0.9498    0.9501       639
     <booktitle>     0.7569    0.7593    0.7572       118
 <collaboration>     0.9636    0.8083    0.8777        12
          <date>     0.9854    0.9858    0.9856       699
        <editor>     0.6666    0.6357    0.6496        14
   <institution>     0.7537    0.8211    0.7857        19
         <issue>     0.9404    0.9200    0.9300        70
       <journal>     0.9405    0.9479    0.9442       537
      <location>     0.8841    0.8606    0.8722        94
          <note>     0.8178    0.6769    0.7404        39
         <pages>     0.9792    0.9870    0.9831       576
     <publisher>     0.9329    0.9380    0.9352        50
        <pubnum>     0.9531    0.9147    0.9335       102
        <series>     0.1500    0.1000    0.1167         2
          <tech>     0.6913    0.7647    0.7251        17
         <title>     0.9572    0.9549    0.9560       457
        <volume>     0.9672    0.9853    0.9762       532
           <web>     0.9038    0.9250    0.9140        12

all (micro avg.)     0.9499    0.9511    0.9505 



> python3 delft/applications/grobidTagger.py citation train_eval --architecture BidLSTM_CRF_FEATURES --embedding glove-840B --fold-count 10 --input data/sequenceLabelling/grobid/citation/citation-060518.train

training runtime: 40409.756 seconds 

Evaluation:

------------------------ fold 0 --------------------------------------
    f1 (micro): 95.09
                  precision    recall  f1-score   support

        <author>     0.9498    0.9484    0.9491       639
     <booktitle>     0.7541    0.7797    0.7667       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9858    0.9900    0.9879       699
        <editor>     0.6429    0.6429    0.6429        14
   <institution>     0.7143    0.7895    0.7500        19
         <issue>     0.9412    0.9143    0.9275        70
       <journal>     0.9410    0.9497    0.9453       537
      <location>     0.8804    0.8617    0.8710        94
          <note>     0.8485    0.7179    0.7778        39
         <pages>     0.9777    0.9896    0.9836       576
     <publisher>     0.9388    0.9200    0.9293        50
        <pubnum>     0.9286    0.8922    0.9100       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.6842    0.7647    0.7222        17
         <title>     0.9560    0.9519    0.9539       457
        <volume>     0.9741    0.9887    0.9813       532
           <web>     0.8462    0.9167    0.8800        12

all (micro avg.)     0.9495    0.9524    0.9509      3989


------------------------ fold 1 --------------------------------------
    f1 (micro): 94.71
                  precision    recall  f1-score   support

        <author>     0.9420    0.9405    0.9413       639
     <booktitle>     0.7258    0.7627    0.7438       118
 <collaboration>     1.0000    0.7500    0.8571        12
          <date>     0.9871    0.9857    0.9864       699
        <editor>     0.7692    0.7143    0.7407        14
   <institution>     0.7143    0.7895    0.7500        19
         <issue>     0.9403    0.9000    0.9197        70
       <journal>     0.9428    0.9516    0.9472       537
      <location>     0.8778    0.8404    0.8587        94
          <note>     0.7429    0.6667    0.7027        39
         <pages>     0.9693    0.9861    0.9776       576
     <publisher>     0.9388    0.9200    0.9293        50
        <pubnum>     0.9495    0.9216    0.9353       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7647    0.7647    0.7647        17
         <title>     0.9538    0.9497    0.9518       457
        <volume>     0.9650    0.9850    0.9749       532
           <web>     0.9167    0.9167    0.9167        12

all (micro avg.)     0.9457    0.9484    0.9471      3989


------------------------ fold 2 --------------------------------------
    f1 (micro): 94.97
                  precision    recall  f1-score   support

        <author>     0.9577    0.9577    0.9577       639
     <booktitle>     0.7541    0.7797    0.7667       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9885    0.9871    0.9878       699
        <editor>     0.6000    0.6429    0.6207        14
   <institution>     0.6190    0.6842    0.6500        19
         <issue>     0.9167    0.9429    0.9296        70
       <journal>     0.9339    0.9479    0.9409       537
      <location>     0.8889    0.8511    0.8696        94
          <note>     0.7941    0.6923    0.7397        39
         <pages>     0.9659    0.9844    0.9751       576
     <publisher>     0.9200    0.9200    0.9200        50
        <pubnum>     0.9583    0.9020    0.9293       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7647    0.7647    0.7647        17
         <title>     0.9558    0.9475    0.9516       457
        <volume>     0.9721    0.9831    0.9776       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9483    0.9511    0.9497      3989


------------------------ fold 3 --------------------------------------
    f1 (micro): 94.97
                  precision    recall  f1-score   support

        <author>     0.9437    0.9452    0.9445       639
     <booktitle>     0.7479    0.7542    0.7511       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9886    0.9900    0.9893       699
        <editor>     0.7692    0.7143    0.7407        14
   <institution>     0.7500    0.7895    0.7692        19
         <issue>     0.9701    0.9286    0.9489        70
       <journal>     0.9291    0.9516    0.9402       537
      <location>     0.8901    0.8617    0.8757        94
          <note>     0.7879    0.6667    0.7222        39
         <pages>     0.9777    0.9896    0.9836       576
     <publisher>     0.9583    0.9200    0.9388        50
        <pubnum>     0.9592    0.9216    0.9400       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.6842    0.7647    0.7222        17
         <title>     0.9578    0.9431    0.9504       457
        <volume>     0.9686    0.9868    0.9777       532
           <web>     0.7692    0.8333    0.8000        12

all (micro avg.)     0.9487    0.9506    0.9497      3989


------------------------ fold 4 --------------------------------------
    f1 (micro): 95.01
                  precision    recall  f1-score   support

        <author>     0.9530    0.9515    0.9522       639
     <booktitle>     0.7311    0.7373    0.7342       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9899    0.9857    0.9878       699
        <editor>     0.6250    0.7143    0.6667        14
   <institution>     0.7273    0.8421    0.7805        19
         <issue>     0.9701    0.9286    0.9489        70
       <journal>     0.9375    0.9497    0.9436       537
      <location>     0.8977    0.8404    0.8681        94
          <note>     0.7632    0.7436    0.7532        39
         <pages>     0.9760    0.9878    0.9819       576
     <publisher>     0.9216    0.9400    0.9307        50
        <pubnum>     0.9300    0.9118    0.9208       102
        <series>     0.2500    0.5000    0.3333         2
          <tech>     0.6667    0.7059    0.6857        17
         <title>     0.9665    0.9475    0.9569       457
        <volume>     0.9703    0.9831    0.9767       532
           <web>     0.9167    0.9167    0.9167        12

all (micro avg.)     0.9497    0.9506    0.9501      3989


------------------------ fold 5 --------------------------------------
    f1 (micro): 94.82
                  precision    recall  f1-score   support

        <author>     0.9422    0.9437    0.9429       639
     <booktitle>     0.7632    0.7373    0.7500       118
 <collaboration>     1.0000    0.7500    0.8571        12
          <date>     0.9801    0.9886    0.9843       699
        <editor>     0.6923    0.6429    0.6667        14
   <institution>     0.7895    0.7895    0.7895        19
         <issue>     0.9701    0.9286    0.9489        70
       <journal>     0.9303    0.9441    0.9372       537
      <location>     0.8913    0.8723    0.8817        94
          <note>     0.8065    0.6410    0.7143        39
         <pages>     0.9777    0.9896    0.9836       576
     <publisher>     0.9600    0.9600    0.9600        50
        <pubnum>     0.9293    0.9020    0.9154       102
        <series>     0.2500    0.5000    0.3333         2
          <tech>     0.6842    0.7647    0.7222        17
         <title>     0.9561    0.9540    0.9551       457
        <volume>     0.9667    0.9831    0.9748       532
           <web>     0.9167    0.9167    0.9167        12

all (micro avg.)     0.9470    0.9494    0.9482      3989


------------------------ fold 6 --------------------------------------
    f1 (micro): 95.10
                  precision    recall  f1-score   support

        <author>     0.9469    0.9484    0.9476       639
     <booktitle>     0.8000    0.7458    0.7719       118
 <collaboration>     1.0000    0.8333    0.9091        12
          <date>     0.9830    0.9914    0.9872       699
        <editor>     0.6154    0.5714    0.5926        14
   <institution>     0.7500    0.7895    0.7692        19
         <issue>     0.9412    0.9143    0.9275        70
       <journal>     0.9341    0.9497    0.9418       537
      <location>     0.8791    0.8511    0.8649        94
          <note>     0.8065    0.6410    0.7143        39
         <pages>     0.9810    0.9878    0.9844       576
     <publisher>     0.9020    0.9200    0.9109        50
        <pubnum>     0.9490    0.9118    0.9300       102
        <series>     0.5000    0.5000    0.5000         2
          <tech>     0.7222    0.7647    0.7429        17
         <title>     0.9564    0.9606    0.9585       457
        <volume>     0.9704    0.9850    0.9776       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9504    0.9516    0.9510      3989


------------------------ fold 7 --------------------------------------
    f1 (micro): 94.70
                  precision    recall  f1-score   support

        <author>     0.9330    0.9374    0.9352       639
     <booktitle>     0.7521    0.7712    0.7615       118
 <collaboration>     0.9000    0.7500    0.8182        12
          <date>     0.9801    0.9871    0.9836       699
        <editor>     0.5556    0.3571    0.4348        14
   <institution>     0.6667    0.7368    0.7000        19
         <issue>     0.9420    0.9286    0.9353        70
       <journal>     0.9376    0.9516    0.9445       537
      <location>     0.8889    0.8511    0.8696        94
          <note>     0.7941    0.6923    0.7397        39
         <pages>     0.9793    0.9844    0.9818       576
     <publisher>     0.9216    0.9400    0.9307        50
        <pubnum>     0.9588    0.9118    0.9347       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7000    0.8235    0.7568        17
         <title>     0.9645    0.9519    0.9581       457
        <volume>     0.9721    0.9812    0.9766       532
           <web>     0.9167    0.9167    0.9167        12

all (micro avg.)     0.9464    0.9476    0.9470      3989


------------------------ fold 8 --------------------------------------
    f1 (micro): 95.22
                  precision    recall  f1-score   support

        <author>     0.9468    0.9468    0.9468       639
     <booktitle>     0.7928    0.7458    0.7686       118
 <collaboration>     1.0000    0.7500    0.8571        12
          <date>     0.9858    0.9900    0.9879       699
        <editor>     0.5714    0.5714    0.5714        14
   <institution>     0.7273    0.8421    0.7805        19
         <issue>     0.9701    0.9286    0.9489        70
       <journal>     0.9394    0.9534    0.9464       537
      <location>     0.8764    0.8298    0.8525        94
          <note>     0.8235    0.7179    0.7671        39
         <pages>     0.9794    0.9913    0.9853       576
     <publisher>     0.9020    0.9200    0.9109        50
        <pubnum>     0.9495    0.9216    0.9353       102
        <series>     0.3333    0.5000    0.4000         2
          <tech>     0.7500    0.7059    0.7273        17
         <title>     0.9564    0.9606    0.9585       457
        <volume>     0.9776    0.9831    0.9803       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9521    0.9524    0.9522      3989


------------------------ fold 9 --------------------------------------
    f1 (micro): 95.16
                  precision    recall  f1-score   support

        <author>     0.9514    0.9499    0.9507       639
     <booktitle>     0.7480    0.7797    0.7635       118
 <collaboration>     1.0000    0.7500    0.8571        12
          <date>     0.9885    0.9857    0.9871       699
        <editor>     0.6000    0.6429    0.6207        14
   <institution>     0.7273    0.8421    0.7805        19
         <issue>     0.9697    0.9143    0.9412        70
       <journal>     0.9464    0.9534    0.9499       537
      <location>     0.8681    0.8404    0.8541        94
          <note>     0.8387    0.6667    0.7429        39
         <pages>     0.9760    0.9878    0.9819       576
     <publisher>     0.9200    0.9200    0.9200        50
        <pubnum>     0.9490    0.9118    0.9300       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.6190    0.7647    0.6842        17
         <title>     0.9690    0.9562    0.9626       457
        <volume>     0.9669    0.9868    0.9767       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9512    0.9521    0.9516      3989

----------------------------------------------------------------------

** Worst ** model scores - run 7
                  precision    recall  f1-score   support

        <author>     0.9330    0.9374    0.9352       639
     <booktitle>     0.7521    0.7712    0.7615       118
 <collaboration>     0.9000    0.7500    0.8182        12
          <date>     0.9801    0.9871    0.9836       699
        <editor>     0.5556    0.3571    0.4348        14
   <institution>     0.6667    0.7368    0.7000        19
         <issue>     0.9420    0.9286    0.9353        70
       <journal>     0.9376    0.9516    0.9445       537
      <location>     0.8889    0.8511    0.8696        94
          <note>     0.7941    0.6923    0.7397        39
         <pages>     0.9793    0.9844    0.9818       576
     <publisher>     0.9216    0.9400    0.9307        50
        <pubnum>     0.9588    0.9118    0.9347       102
        <series>     0.0000    0.0000    0.0000         2
          <tech>     0.7000    0.8235    0.7568        17
         <title>     0.9645    0.9519    0.9581       457
        <volume>     0.9721    0.9812    0.9766       532
           <web>     0.9167    0.9167    0.9167        12

all (micro avg.)     0.9464    0.9476    0.9470      3989


** Best ** model scores - run 8
                  precision    recall  f1-score   support

        <author>     0.9468    0.9468    0.9468       639
     <booktitle>     0.7928    0.7458    0.7686       118
 <collaboration>     1.0000    0.7500    0.8571        12
          <date>     0.9858    0.9900    0.9879       699
        <editor>     0.5714    0.5714    0.5714        14
   <institution>     0.7273    0.8421    0.7805        19
         <issue>     0.9701    0.9286    0.9489        70
       <journal>     0.9394    0.9534    0.9464       537
      <location>     0.8764    0.8298    0.8525        94
          <note>     0.8235    0.7179    0.7671        39
         <pages>     0.9794    0.9913    0.9853       576
     <publisher>     0.9020    0.9200    0.9109        50
        <pubnum>     0.9495    0.9216    0.9353       102
        <series>     0.3333    0.5000    0.4000         2
          <tech>     0.7500    0.7059    0.7273        17
         <title>     0.9564    0.9606    0.9585       457
        <volume>     0.9776    0.9831    0.9803       532
           <web>     1.0000    1.0000    1.0000        12

all (micro avg.)     0.9521    0.9524    0.9522      3989

----------------------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

        <author>     0.9467    0.9469    0.9468       639
     <booktitle>     0.7569    0.7593    0.7578       118
 <collaboration>     0.9900    0.7917    0.8792        12
          <date>     0.9857    0.9881    0.9869       699
        <editor>     0.6441    0.6214    0.6298        14
   <institution>     0.7186    0.7895    0.7519        19
         <issue>     0.9532    0.9229    0.9376        70
       <journal>     0.9372    0.9503    0.9437       537
      <location>     0.8839    0.8500    0.8666        94
          <note>     0.8006    0.6846    0.7374        39
         <pages>     0.9760    0.9878    0.9819       576
     <publisher>     0.9283    0.9280    0.9281        50
        <pubnum>     0.9461    0.9108    0.9281       102
        <series>     0.1333    0.2000    0.1567         2
          <tech>     0.7040    0.7588    0.7293        17
         <title>     0.9593    0.9523    0.9557       457
        <volume>     0.9704    0.9846    0.9774       532
           <web>     0.9282    0.9417    0.9347        12

all (micro avg.)     0.9489    0.9506    0.9498 



affiliation-address
===================



> python3 delft/applications/grobidTagger.py affiliation-address train_eval --architecture BidLSTM_CRF_FEATURES --embedding glove-840B --input data/sequenceLabelling/grobid/affiliation-address/affiliation-address-060518.train --fold-count 10

training runtime: 12045.537 seconds 

Evaluation:

------------------------ fold 0 --------------------------------------
    f1 (micro): 87.35
                  precision    recall  f1-score   support

      <addrLine>     0.7742    0.7742    0.7742        62
       <country>     0.9752    0.9937    0.9843       158
    <department>     0.8084    0.8036    0.8060       168
   <institution>     0.8223    0.8257    0.8240       241
    <laboratory>     0.6389    0.6571    0.6479        35
        <marker>     0.9691    0.9895    0.9792        95
       <postBox>     0.8333    0.8333    0.8333         6
      <postCode>     0.9211    0.9130    0.9170       115
        <region>     0.8889    0.8571    0.8727        56
    <settlement>     0.8964    0.9010    0.8987       192

all (micro avg.)     0.8719    0.8750    0.8735      1128


------------------------ fold 1 --------------------------------------
    f1 (micro): 85.84
                  precision    recall  f1-score   support

      <addrLine>     0.7581    0.7581    0.7581        62
       <country>     0.9568    0.9810    0.9688       158
    <department>     0.8250    0.7857    0.8049       168
   <institution>     0.8017    0.8050    0.8033       241
    <laboratory>     0.5333    0.6857    0.6000        35
        <marker>     0.9688    0.9789    0.9738        95
       <postBox>     0.5714    0.6667    0.6154         6
      <postCode>     0.8879    0.8957    0.8918       115
        <region>     0.8621    0.8929    0.8772        56
    <settlement>     0.9130    0.8750    0.8936       192

all (micro avg.)     0.8569    0.8599    0.8584      1128


------------------------ fold 2 --------------------------------------
    f1 (micro): 87.81
                  precision    recall  f1-score   support

      <addrLine>     0.7500    0.7742    0.7619        62
       <country>     0.9627    0.9810    0.9718       158
    <department>     0.7989    0.8512    0.8242       168
   <institution>     0.8397    0.8257    0.8326       241
    <laboratory>     0.7931    0.6571    0.7188        35
        <marker>     0.9691    0.9895    0.9792        95
       <postBox>     0.6667    0.6667    0.6667         6
      <postCode>     0.9292    0.9130    0.9211       115
        <region>     0.8947    0.9107    0.9027        56
    <settlement>     0.8912    0.8958    0.8935       192

all (micro avg.)     0.8750    0.8812    0.8781      1128


------------------------ fold 3 --------------------------------------
    f1 (micro): 87.21
                  precision    recall  f1-score   support

      <addrLine>     0.7188    0.7419    0.7302        62
       <country>     0.9506    0.9747    0.9625       158
    <department>     0.8161    0.8452    0.8304       168
   <institution>     0.8382    0.8382    0.8382       241
    <laboratory>     0.6364    0.6000    0.6176        35
        <marker>     0.9691    0.9895    0.9792        95
       <postBox>     0.6667    0.6667    0.6667         6
      <postCode>     0.9286    0.9043    0.9163       115
        <region>     0.8704    0.8393    0.8545        56
    <settlement>     0.9096    0.8906    0.9000       192

all (micro avg.)     0.8709    0.8732    0.8721      1128


------------------------ fold 4 --------------------------------------
    f1 (micro): 86.58
                  precision    recall  f1-score   support

      <addrLine>     0.7377    0.7258    0.7317        62
       <country>     0.9500    0.9620    0.9560       158
    <department>     0.8057    0.8393    0.8222       168
   <institution>     0.8235    0.8133    0.8184       241
    <laboratory>     0.6667    0.6286    0.6471        35
        <marker>     0.9691    0.9895    0.9792        95
       <postBox>     0.6667    0.6667    0.6667         6
      <postCode>     0.9115    0.8957    0.9035       115
        <region>     0.8889    0.8571    0.8727        56
    <settlement>     0.8958    0.8958    0.8958       192

all (micro avg.)     0.8654    0.8661    0.8658      1128


------------------------ fold 5 --------------------------------------
    f1 (micro): 86.38
                  precision    recall  f1-score   support

      <addrLine>     0.7778    0.7903    0.7840        62
       <country>     0.9568    0.9810    0.9688       158
    <department>     0.8012    0.7917    0.7964       168
   <institution>     0.8082    0.8216    0.8148       241
    <laboratory>     0.6129    0.5429    0.5758        35
        <marker>     0.9688    0.9789    0.9738        95
       <postBox>     0.5714    0.6667    0.6154         6
      <postCode>     0.9123    0.9043    0.9083       115
        <region>     0.8167    0.8750    0.8448        56
    <settlement>     0.9105    0.9010    0.9058       192

all (micro avg.)     0.8616    0.8661    0.8638      1128


------------------------ fold 6 --------------------------------------
    f1 (micro): 86.55
                  precision    recall  f1-score   support

      <addrLine>     0.7500    0.7742    0.7619        62
       <country>     0.9625    0.9747    0.9686       158
    <department>     0.8160    0.7917    0.8036       168
   <institution>     0.7968    0.8299    0.8130       241
    <laboratory>     0.6667    0.6286    0.6471        35
        <marker>     0.9691    0.9895    0.9792        95
       <postBox>     0.5714    0.6667    0.6154         6
      <postCode>     0.9469    0.9304    0.9386       115
        <region>     0.8491    0.8036    0.8257        56
    <settlement>     0.8953    0.8906    0.8930       192

all (micro avg.)     0.8640    0.8670    0.8655      1128


------------------------ fold 7 --------------------------------------
    f1 (micro): 86.83
                  precision    recall  f1-score   support

      <addrLine>     0.7424    0.7903    0.7656        62
       <country>     0.9503    0.9684    0.9592       158
    <department>     0.8000    0.8333    0.8163       168
   <institution>     0.8216    0.8216    0.8216       241
    <laboratory>     0.6286    0.6286    0.6286        35
        <marker>     0.9792    0.9895    0.9843        95
       <postBox>     0.8333    0.8333    0.8333         6
      <postCode>     0.9204    0.9043    0.9123       115
        <region>     0.9020    0.8214    0.8598        56
    <settlement>     0.9000    0.8906    0.8953       192

all (micro avg.)     0.8660    0.8706    0.8683      1128


------------------------ fold 8 --------------------------------------
    f1 (micro): 85.89
                  precision    recall  f1-score   support

      <addrLine>     0.7385    0.7742    0.7559        62
       <country>     0.9565    0.9747    0.9655       158
    <department>     0.7964    0.7917    0.7940       168
   <institution>     0.7958    0.7925    0.7942       241
    <laboratory>     0.6316    0.6857    0.6575        35
        <marker>     0.9691    0.9895    0.9792        95
       <postBox>     0.5714    0.6667    0.6154         6
      <postCode>     0.9545    0.9130    0.9333       115
        <region>     0.8276    0.8571    0.8421        56
    <settlement>     0.8782    0.9010    0.8895       192

all (micro avg.)     0.8544    0.8635    0.8589      1128


------------------------ fold 9 --------------------------------------
    f1 (micro): 85.92
                  precision    recall  f1-score   support

      <addrLine>     0.7619    0.7742    0.7680        62
       <country>     0.9506    0.9747    0.9625       158
    <department>     0.7879    0.7738    0.7808       168
   <institution>     0.8000    0.8133    0.8066       241
    <laboratory>     0.6316    0.6857    0.6575        35
        <marker>     0.9688    0.9789    0.9738        95
       <postBox>     0.5714    0.6667    0.6154         6
      <postCode>     0.9211    0.9130    0.9170       115
        <region>     0.8571    0.8571    0.8571        56
    <settlement>     0.8953    0.8906    0.8930       192

all (micro avg.)     0.8558    0.8626    0.8592      1128

----------------------------------------------------------------------

** Worst ** model scores - run 1
                  precision    recall  f1-score   support

      <addrLine>     0.7581    0.7581    0.7581        62
       <country>     0.9568    0.9810    0.9688       158
    <department>     0.8250    0.7857    0.8049       168
   <institution>     0.8017    0.8050    0.8033       241
    <laboratory>     0.5333    0.6857    0.6000        35
        <marker>     0.9688    0.9789    0.9738        95
       <postBox>     0.5714    0.6667    0.6154         6
      <postCode>     0.8879    0.8957    0.8918       115
        <region>     0.8621    0.8929    0.8772        56
    <settlement>     0.9130    0.8750    0.8936       192

all (micro avg.)     0.8569    0.8599    0.8584      1128


** Best ** model scores - run 2
                  precision    recall  f1-score   support

      <addrLine>     0.7500    0.7742    0.7619        62
       <country>     0.9627    0.9810    0.9718       158
    <department>     0.7989    0.8512    0.8242       168
   <institution>     0.8397    0.8257    0.8326       241
    <laboratory>     0.7931    0.6571    0.7188        35
        <marker>     0.9691    0.9895    0.9792        95
       <postBox>     0.6667    0.6667    0.6667         6
      <postCode>     0.9292    0.9130    0.9211       115
        <region>     0.8947    0.9107    0.9027        56
    <settlement>     0.8912    0.8958    0.8935       192

all (micro avg.)     0.8750    0.8812    0.8781      1128

----------------------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

      <addrLine>     0.7509    0.7677    0.7591        62
       <country>     0.9572    0.9766    0.9668       158
    <department>     0.8056    0.8107    0.8079       168
   <institution>     0.8148    0.8187    0.8167       241
    <laboratory>     0.6440    0.6400    0.6398        35
        <marker>     0.9700    0.9863    0.9781        95
       <postBox>     0.6524    0.7000    0.6744         6
      <postCode>     0.9233    0.9087    0.9159       115
        <region>     0.8657    0.8571    0.8609        56
    <settlement>     0.8985    0.8932    0.8958       192

all (micro avg.)     0.8642    0.8685    0.8663 