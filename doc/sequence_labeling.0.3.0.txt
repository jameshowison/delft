



python3 delft/applications/nerTagger.py train_eval --dataset-type conll2003 --architecture BidLSTM_CRF --embedding glove-840B --fold-count 10

training runtime: 29956.487 seconds 

Evaluation on test set:

------------------------ fold 0 --------------------------------------
    f1 (micro): 91.01
                  precision    recall  f1-score   support

             LOC     0.9153    0.9394    0.9272      1668
            MISC     0.8023    0.7977    0.8000       702
             ORG     0.8982    0.8820    0.8900      1661
             PER     0.9610    0.9598    0.9604      1617

all (micro avg.)     0.9095    0.9108    0.9101      5648


------------------------ fold 1 --------------------------------------
    f1 (micro): 91.38
                  precision    recall  f1-score   support

             LOC     0.9270    0.9371    0.9320      1668
            MISC     0.8127    0.8034    0.8080       702
             ORG     0.8931    0.8898    0.8914      1661
             PER     0.9606    0.9660    0.9633      1617

all (micro avg.)     0.9127    0.9148    0.9138      5648


------------------------ fold 2 --------------------------------------
    f1 (micro): 90.97
                  precision    recall  f1-score   support

             LOC     0.9204    0.9359    0.9281      1668
            MISC     0.8107    0.7806    0.7954       702
             ORG     0.8852    0.8910    0.8881      1661
             PER     0.9657    0.9579    0.9618      1617

all (micro avg.)     0.9097    0.9097    0.9097      5648


------------------------ fold 3 --------------------------------------
    f1 (micro): 90.97
                  precision    recall  f1-score   support

             LOC     0.9110    0.9329    0.9218      1668
            MISC     0.8290    0.7806    0.8041       702
             ORG     0.8828    0.8983    0.8905      1661
             PER     0.9628    0.9604    0.9616      1617

all (micro avg.)     0.9078    0.9117    0.9097      5648


------------------------ fold 4 --------------------------------------
    f1 (micro): 90.92
                  precision    recall  f1-score   support

             LOC     0.9149    0.9347    0.9247      1668
            MISC     0.8356    0.7892    0.8117       702
             ORG     0.8809    0.8904    0.8856      1661
             PER     0.9535    0.9629    0.9582      1617

all (micro avg.)     0.9067    0.9117    0.9092      5648


------------------------ fold 5 --------------------------------------
    f1 (micro): 91.25
                  precision    recall  f1-score   support

             LOC     0.9189    0.9371    0.9279      1668
            MISC     0.8076    0.8191    0.8133       702
             ORG     0.8934    0.8880    0.8907      1661
             PER     0.9600    0.9641    0.9620      1617

all (micro avg.)     0.9093    0.9157    0.9125      5648


------------------------ fold 6 --------------------------------------
    f1 (micro): 91.03
                  precision    recall  f1-score   support

             LOC     0.9162    0.9371    0.9265      1668
            MISC     0.8109    0.8063    0.8086       702
             ORG     0.8879    0.8826    0.8853      1661
             PER     0.9664    0.9604    0.9634      1617

all (micro avg.)     0.9092    0.9115    0.9103      5648


------------------------ fold 7 --------------------------------------
    f1 (micro): 91.07
                  precision    recall  f1-score   support

             LOC     0.9233    0.9376    0.9304      1668
            MISC     0.8126    0.8091    0.8108       702
             ORG     0.8826    0.8874    0.8850      1661
             PER     0.9604    0.9598    0.9601      1617

all (micro avg.)     0.9083    0.9132    0.9107      5648


------------------------ fold 8 --------------------------------------
    f1 (micro): 90.90
                  precision    recall  f1-score   support

             LOC     0.9108    0.9365    0.9234      1668
            MISC     0.8129    0.8048    0.8089       702
             ORG     0.8964    0.8808    0.8886      1661
             PER     0.9551    0.9604    0.9578      1617

all (micro avg.)     0.9074    0.9106    0.9090      5648


------------------------ fold 9 --------------------------------------
    f1 (micro): 90.78
                  precision    recall  f1-score   support

             LOC     0.9198    0.9347    0.9271      1668
            MISC     0.8200    0.7721    0.7953       702
             ORG     0.8729    0.8928    0.8827      1661
             PER     0.9616    0.9604    0.9610      1617

all (micro avg.)     0.9060    0.9095    0.9078      5648

----------------------------------------------------------------------

** Worst ** model scores - run 9
                  precision    recall  f1-score   support

             LOC     0.9198    0.9347    0.9271      1668
            MISC     0.8200    0.7721    0.7953       702
             ORG     0.8729    0.8928    0.8827      1661
             PER     0.9616    0.9604    0.9610      1617

all (micro avg.)     0.9060    0.9095    0.9078      5648


** Best ** model scores - run 1
                  precision    recall  f1-score   support

             LOC     0.9270    0.9371    0.9320      1668
            MISC     0.8127    0.8034    0.8080       702
             ORG     0.8931    0.8898    0.8914      1661
             PER     0.9606    0.9660    0.9633      1617

all (micro avg.)     0.9127    0.9148    0.9138      5648

----------------------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

             LOC     0.9178    0.9363    0.9269      1668
            MISC     0.8154    0.7963    0.8056       702
             ORG     0.8873    0.8883    0.8878      1661
             PER     0.9607    0.9612    0.9610      1617

all (micro avg.)     0.9087    0.9119    0.9103




















> python3 delft/applications/nerTagger.py train_eval --dataset-type conll2003 --architecture BERT --transformer bert-base-cased --fold-count 10

training runtime: 11087.346 seconds 

Evaluation on test set:

------------------------ fold 0 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights0.hdf5
number of alignment issues with test set: 3390
you might need to increase the maximum sequence input length of the model and retrain
    f1: 90.94
    precision: 90.22
    recall: 91.66

------------------------ fold 1 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights1.hdf5
number of alignment issues with test set: 3176
you might need to increase the maximum sequence input length of the model and retrain
    f1: 91.16
    precision: 90.29
    recall: 92.05

------------------------ fold 2 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights2.hdf5
number of alignment issues with test set: 3276
you might need to increase the maximum sequence input length of the model and retrain
    f1: 91.29
    precision: 90.49
    recall: 92.10

------------------------ fold 3 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights3.hdf5
number of alignment issues with test set: 3403
you might need to increase the maximum sequence input length of the model and retrain
    f1: 91.18
    precision: 90.35
    recall: 92.03

------------------------ fold 4 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights4.hdf5
number of alignment issues with test set: 3403
you might need to increase the maximum sequence input length of the model and retrain
    f1: 90.96
    precision: 90.14
    recall: 91.80

------------------------ fold 5 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights5.hdf5
number of alignment issues with test set: 3442
you might need to increase the maximum sequence input length of the model and retrain
    f1: 90.93
    precision: 90.15
    recall: 91.73

------------------------ fold 6 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights6.hdf5
number of alignment issues with test set: 3401
you might need to increase the maximum sequence input length of the model and retrain
    f1: 90.79
    precision: 89.88
    recall: 91.71

------------------------ fold 7 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights7.hdf5
number of alignment issues with test set: 3401
you might need to increase the maximum sequence input length of the model and retrain
    f1: 91.17
    precision: 90.39
    recall: 91.96

------------------------ fold 8 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights8.hdf5
number of alignment issues with test set: 3403
you might need to increase the maximum sequence input length of the model and retrain
    f1: 90.80
    precision: 89.83
    recall: 91.78

------------------------ fold 9 --------------------------------------
loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights9.hdf5
number of alignment issues with test set: 3415
you might need to increase the maximum sequence input length of the model and retrain
    f1: 91.07
    precision: 90.32
    recall: 91.84
----------------------------------------------------------------------

** Worst ** model scores - run 6
                  precision    recall  f1-score   support

             LOC     0.9197    0.9269    0.9233      1668
            MISC     0.7651    0.8120    0.7878       702
             ORG     0.8778    0.9085    0.8929      1661
             PER     0.9611    0.9617    0.9614      1617

all (micro avg.)     0.8988    0.9171    0.9079      5648


** Best ** model scores - run 2
                  precision    recall  f1-score   support

             LOC     0.9311    0.9317    0.9314      1668
            MISC     0.7807    0.8319    0.8055       702
             ORG     0.8863    0.9061    0.8961      1661
             PER     0.9541    0.9641    0.9591      1617

all (micro avg.)     0.9049    0.9210    0.9129      5648

loading model weights data/models/sequenceLabelling/ner-en-conll2003-BERT/model_weights2.hdf5
----------------------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

             LOC     0.9233    0.9312    0.9272      1668
            MISC     0.7739    0.8239    0.7981       702
             ORG     0.8824    0.9042    0.8932      1661
             PER     0.9600    0.9618    0.9609      1617

all (micro avg.)     0.9021    0.9187    0.9103        











